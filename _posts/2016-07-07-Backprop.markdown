---
layout: post
title: Backpropagation Demystified 
date: 2016-09-09
---

The main goal of training a neural network is to determine the value of each connection weight which minimize the specified cost function. Backpropagation is the standard way of training neural network where weights of the neural network connections are updated as follows

$$\large w_i := w_i - \alpha *  \frac{\partial E(w_i)}{\partial w_i}$$ 

Learning rate controls how much we want to learn in each step of training process. Learning rate is determined empirically. 

Partial derivative of cost function w.r.t. weight gives the direction in which weight should be changed to improve the performance of the system. Above expression shows that to update weight of the connection, the partial derivative of cost function $E(w_i)$ with respect to the weight $$w_i$$ need to be computed. **Backpropagation is an efficient way to compute partial derivative of error w.r.t. each connection weight of the neural network**.


### Refresher: Intuitive meaning of derivative and partial derivative 
Let's assume $$x_1$$ is time allocated by a man for exercise and $$y$$ is the weight of the man. The derivative of $$y$$  w.r.t.  $$x_1$$ means if the man makes a small change in the time allocated for the exercise how much his weight will change. Mathematically, the derivative of $$y$$ w.r.t.  $$x_1$$ is represented as $$\frac{dy}{dx_1}$$

We know that diet ($$x_2$$) also has an effect on the weight of the person.  Analysis of change in the weight of the person w.r.t. a  small change in the exercise keeping diet same is given by the partial derivative of $$y$$ &nbsp; w.r.t. &nbsp; $$x_1$$. Mathematically, the partial derivative of $$y$$ w.r.t. $$\large x_1$$ is represented as $$\frac{\partial y}{\partial x_1}$$. Similarly, if the person keeps the time of exercise $$x_1$$ same and makes a small change in the diet $$x_2$$ it is represented as the partial derivative of $$y$$ w.r.t. $$x_2$$. Partial derivative of $$y$$ &nbsp; w.r.t. &nbsp; $$x_2$$ is represented as $$\frac{\partial y}{\partial x_2}$$ 


  ![raw](/assets/img/nn/raw-nn.png){: .center-image}
<center><h4>Figure 1: Toy neural network</h4></center>

Figure 1 shows one of the simplest possible neural network which is good enough to understand details of the backpropagation algorithm. We consider input-layer as layer-1 ($$L_1$$), hidden-layer as ($$L_2$$) and output-layer as ($$L_3$$). $$ x_1$$  and $$ x_2$$ are input, blue nodes are nodes of hidden-layer and the red node is output-node. There are 6 connection weights $$ w_1$$ to $$ w_6$$.

#### We want to minimize the square of error

 Square Error  (E) = $$\frac{(y-\hat{y})^2}{2}$$

*Square of error is divided by 2 just to make computation clean and we know that constant does not affect optimization problem.*


A node (blue/red) in Figure 1 represents three computations namely,  multiplication of an input and a weight, sum of all the results of multiplications and finally activation score which is non-linearity of the summation. To have a better insight about the neural network, we redraw a detailed diagram in Figure 2. Multiplier, summation and non-linearity function in Figure 2 is equivalent to a node in Figure 1. 

  ![raw](/assets/img/nn/nn-detail.png){: .center-image}
<center><h4>Figure 2: Equivalent detailed view of the toy neural network</h4></center>

The way to read $$ z_1^{(2)}$$ is z of  node-1 in layer-2 [**Note: subscript => item number and superscript => layer-number**] . $$ a_1^{(2)}$$ is the activation score of node-1 in layer-2. Following expressions can be obtained from figure 2

$$ z_1^{(2)} = x_1 * w_1 + x_2 * w_3 $$
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
$$  z_2^{(2)} = x_1 * w_2 + x_2 * w_4$$
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
$$  a_1^{(2)} = f(z_1^{(2)})$$  


$$  a_2^{(2)} = f(z_2^{(2)})$$
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
$$  z_1^{(3)} = a_1^{(2)} * w_5 + a_1^{(2)} * w_6$$
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
$$  a_1^{(3)} = f(z_1^{(3)}) = \hat{y}$$
where, f() is a non-linearity function. Usually, non-linear function f() are **sigmoid, tanh or Relu**.

Backpropagation is all about computing partital derivative efficiently. Using backpropagation,  we can calculate $$ \frac{\partial E}{\partial w_i}$$ effectively where i = {1, 2, 3, 4, 5, 6}. 

The partial derivative of error with respect to weight gives **amount by which error will change if there is a small change in the weight**. To calcualate the partial derivative of error w.r.t. weight  $$ \frac{\partial E}{\partial w_1}$$ **chain rule** is used.  

$$ \frac{\partial E}{\partial w_1} = \frac{\partial E}{\partial a_1^{(3)}} \frac{\partial a_1^{(3)}}{\partial w_1} = \frac{\partial E}{\partial a_1^{(3)}} \frac{\partial a_1^{(3)}}{\partial z_1^{(3)}}   \frac{\partial z_1^{(3)}}{\partial w_1} = \frac{\partial E}{\partial a_1^{(3)}} \frac{\partial a_1^{(3)}}{\partial z_1^{(3)}}   \frac{\partial z_1^{(3)}}{\partial a_1^{(2)}} \frac{\partial a_1^{(2)}}{\partial w_1}  = \frac{\partial E}{\partial a_1^{(3)}} \frac{\partial a_1^{(3)}}{\partial z_1^{(3)}}   \frac{\partial z_1^{(3)}}{\partial a_1^{(2)}} \frac{\partial a_1^{(2)}}{\partial z_1^{(2)}} \frac{\partial z_1^{(2)}}{\partial w_1}$$ 

To come up with the expression from Figure 1 can be confusing. However, if we construct a detailed view of the neural network as in Figure 2 then getting the expression is simply following chain rule from **output to input** (note it is backward). For $$ w_2$$ to $$ w_6$$, the partial derivatives of error with respect to the weights can be calculated in the same way.

OMG!! expression of such a simple neural network is so complex. There will be total six partial derivatives, so how can we compute them effectively? **The answer is dynamic programming.**  Dynamic programming is the fancy name of ** memorization and reutilization of computation.** We use the **chain rule** to get the expression of the partial derivative of error w.r.t. weights. To compute such complex expressions efficiently  **dynamic programming** is used.


---
#### Layer-3 
$$ E =\frac{(y-\hat{y})^2}{2} =\frac{(y-a_1^{(3)})^2}{2}$$
 

$$ \frac{\partial E}{\partial z_1^{(3)}}  = \frac{\partial E}{\partial a_1^{(3)}} \frac{\partial a_1^{(3)}}{\partial z_1^{(3)}} = \frac{\partial \frac{(y-\hat{y})^2}{2}}{\partial a_1^{(3)}} \frac{\partial a_1^{(3)}}{\partial z_1^{(3)}} = \frac{\partial \frac{(y-a_1^{(3)})^2}{2}}{\partial a_1^{(3)}} \frac{\partial f(z_1^{(3)})}{\partial z_1^{(3)}} = -(y-a_1^{(3)}) \frac{\partial a_1^{(3)}}{\partial z_1^{(3)}}$$ 

$$ = (a_1^{(3)} -y) f'(z_1^{(3)}) = \delta_1^{(3)}$$

For ease of representation we represent $$ \frac{\partial E}{\partial z_1^{(3)}}$$ as $$ \delta_1^{(3)}$$. **Note that $$z$$ and $$\delta$$ have same sub-script (item-number) and superscript (layer-number).**

To evaluate  $$ \frac{\partial E}{\partial w_5}$$, the computation of $$\delta_1^{(3)}$$ can be utilized (memorized and reutilized) hence the repeated computation is avoided.

$$ \frac{\partial E}{\partial w_5}=  \frac{\partial E}{\partial z_1^{(3)}} \frac{\partial z_1^{(3)}}{\partial w_5} = \delta_1^{(3)} \frac{\partial (a_1^{(2)} * w_5 + a_2^{(2)} * w_6)}{\partial w_5} = \delta_1^{(3)} a_1^{(2)}$$
First chain rule is used to obtain the expression and then computaiton of $$\delta_1^{(3)}$$ is utilzed to compute  $$ \frac{\partial E}{\partial w_5}$$. Following the similar steps, $$ \frac{\partial E}{\partial w_6}$$ can be computed.
  $$ \frac{\partial E}{\partial w_6}= \delta_1^{(3)} a_2^{(2)}$$


  ### Layer-2
The computations of layer-3 is reused for the computation of layer 2. Note that, computation starts from output toward input using the chain rule. The reutilization of the computation can be seen as message passing in the backward direction. 

$$ \frac{\partial E}{\partial z_1^{(2)}}  = \frac{\partial E}{\partial z_1^{(3)}} \frac{\partial z_1^{(3)}}{\partial a_1^{(3)}} \frac{\partial a_1^{(2)}}{\partial z_1^{(2)}} = \delta_1^{(3)}  \frac{\partial (a_1^{(2)} * w_5 + a_1^{(2)} * w_6)}{\partial a_1^{(2)}}  \frac{\partial f(z_1^{(2)})}{\partial z_1^{(2)}} = \delta_1^{(3)} * w_5 *f'(z_1^{(2)}) = \delta_1^{(2)}$$

Similarly, 
$$ \frac{\partial E}{\partial z_2^{(2)}} = \delta_1^{(3)} * w_6 * f'(z_2^{(2)}) = \delta_2^{(2)}$$

It is observed that term which appears in the computation of $$ \frac{\partial E}{\partial w_5}$$ will be memorized and reutilized in the computation of  $$ \frac{\partial E}{\partial w_1}$$.

Let's calculate the partial derivative of error with respect to weight. 

$$ \frac{\partial E}{\partial w_1} = \frac{\partial E}{\partial z_1^{(2)}} \frac{\partial  (x_1 * w_1 + x_2 * w_3)}{\partial w_1} =  \delta_1^{(2)} * x_1 $$

let's say $$ x_1 = a_1^{(1)}$$ and  $$ x_2 = a_2^{(1)}$$
Then,  $$ \frac{\partial E}{\partial w_1}  =  \delta_1^{(2)} * a_1^{(1)} $$


We start with  following expression, 
$$ \frac{\partial E}{\partial w_1}  = \frac{\partial E}{\partial a_1^{(3)}} \frac{\partial a_1^{(3)}}{\partial z_1^{(3)}}   \frac{\partial z_1^{(3)}}{\partial a_1^{(2)}} \frac{\partial a_1^{(2)}}{\partial z_1^{(2)}} \frac{\partial z_1^{(2)}}{\partial w_1}$$ which looks like there is a lot of computation. But, utilizing the dynamic programming in each layer computation is minimized. Now,  $$ \frac{\partial E}{\partial w_1}  =  \delta_1^{(2)} * a_1^{(1)} $$ which looks much simpler and also computationally very efficient.

Similarly, the expression for the partial derivative of error w.r.t. remaining weights can be computed. We observed that if we break down a neural network into a detailed diagram as in Figure 2 then it would be easier to come up with **chain rule**. Once we have the expression, the long and complex expression can be computed efficiently using **dynamic programming**.  

