<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge">

  <title>Surendra Sedhai | Backpropagation Demystified</title>
  <meta name="description" content="Lets explore :) .
">

  <link rel="shortcut icon" href="http://localhost:4000/assets/img/favicon.ico">

  <link rel="stylesheet" href="http://localhost:4000/assets/css/main.css">
  <link rel="canonical" href="http://localhost:4000/blog/2017/Backprop/">
</head>


  <body>

    <header class="site-header">

  <div class="wrapper">

    
    <span class="site-title">
        
        <a href="/" style="color:white"><strong>Surendra</strong>  Sedhai</a>
    </span>
    

    <nav class="site-nav" >

      <div class="trigger" style="color:white">
        <!-- About -->
        <a class="page-link"  style="color:white" href="http://localhost:4000/">About</a>

        <!-- Blog -->
        <a class="page-link" style="color:white" href="http://localhost:4000/blog/">Blog</a>

        <!-- Pages -->
        
          
        
          
        
          
        
          
            <a class="page-link"  style="color:white"  href="http://localhost:4000/publications/">Publications</a>
          
        
          
        

        <!-- CV link -->
        <!-- <a class="page-link" href="http://localhost:4000/assets/pdf/CV.pdf">vitae</a> -->

      </div>
    </nav>

  </div>

</header>



    <div class="page-content">
      <div class="wrapper">
        <div class="post">

  <header class="post-header">
    
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>
<script
  type="text/javascript"
  charset="utf-8"
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"
>
</script>
<script
  type="text/javascript"
  charset="utf-8"
  src="https://vincenttam.github.io/javascripts/MathJaxLocal.js"
>
</script>


    <h1 class="post-title">Backpropagation Demystified</h1>
    <p class="post-meta">September 9, 2017</p>
  </header>

  <article class="post-content">
    <p>Backpropagation is a well-established approach to train a neural network. The main goal of training a neural network is to determine the optimum values of connection weights (parameters) of the neural network. Following equation is used to update the connection weights</p>

<script type="math/tex; mode=display">w_i := w_i - \alpha *  \frac{\partial E(w_i)}{\partial w_i}</script>

<p>where, $w_i$ is the connection weight and $\alpha$ is the learning rate. Learning rate $\alpha$ controls magnitude of the change in the connection weight on each step of training process.</p>

<p>In the above equation, the partial derivative of the $E(w_i)$ w.r.t  $w_i$ gives the direction in which the weight should be modified to reduce the error. In the equation, the only computationally expensive operation is to compute partial derivative. Without using backpropagation approach, with the increase in the number of layers of neural network computional complexity of the partial derivative increase exponentially. <strong>Using backprogation technique, the partial derivative of error w.r.t connection weight can be computed efficienly.</strong></p>

<hr />
<h4 id="refresher-intuitive-meaning-of-derivative-and-partial-derivative"><strong>Refresher: Intuitive meaning of derivative and partial derivative</strong></h4>
<p>Let’s assume <script type="math/tex">x_1</script> is time allocated by a person for the exercise and <script type="math/tex">y</script> is the weight of the person. <strong>The derivative of <script type="math/tex">y</script>  w.r.t.  <script type="math/tex">x_1</script> means if the person makes a small change in the time allocated for the exercise how much the weight will change</strong>. Mathematically, the derivative of <script type="math/tex">y</script> w.r.t.  <script type="math/tex">x_1</script> is represented as <script type="math/tex">\frac{dy}{dx_1}</script>.</p>

<p>We know that diet (<script type="math/tex">x_2</script>) also has an effect on the weight of the person.  Analysis of change in the weight of the person w.r.t. a small change in the time allocated for the exercise keeping diet same is given by the partial derivative of <script type="math/tex">y</script>   w.r.t.   <script type="math/tex">x_1</script>. Mathematically, the partial derivative of <script type="math/tex">y</script> w.r.t. <script type="math/tex">x_1</script> is represented as <script type="math/tex">\frac{\partial y}{\partial x_1}</script>. Similarly, if the person keeps the exercise time <script type="math/tex">x_1</script>  the same and makes a small change in the diet <script type="math/tex">x_2</script> it is represented as the partial derivative of <script type="math/tex">y</script> w.r.t. <script type="math/tex">x_2</script> which is represented as <script type="math/tex">\frac{\partial y}{\partial x_2}</script>.</p>

<hr />

<p><img src="/assets/img/nn/raw-nn.png" alt="raw" class="center-image" /></p>
<center><h4>Figure 1: Toy neural network</h4></center>

<p>Figure 1 shows the simplest possible neural network which is good enough to understand details of the backpropagation algorithm. In the figure, input-layer is layer-1 (<script type="math/tex">L_1</script>), hidden-layer is (<script type="math/tex">L_2</script>) and output-layer is (<script type="math/tex">L_3</script>). <script type="math/tex">x_1</script>  and <script type="math/tex">x_2</script> are input to the neural network, blue nodes are nodes of hidden-layer and the red node is output-node. There are 6 connection weights <script type="math/tex">w_1</script> to <script type="math/tex">w_6</script>. In the training process we want to determine values of the connection weights which  minimize error. In this post regression error is minimized to explain the backpropagation technique but the idea remains the same for the classification problem as well.</p>

<p>Square Error  (E) = <script type="math/tex">\frac{(y-\hat{y})^2}{2}</script></p>

<p><em>As constant does not affect optimization problem, square of error is divided by 2 just to make the computation clean.</em></p>

<p>In Figure 1, each node (red/blue) represents three computations namely, multiplication of an input and a weight, sum of all the results of multiplications and finally activation score which is non-linearity of the summation. Figure 2 shows the clear and detailed view of the neural network which consists of multiplier, summation unit and non-linearity unit. Combination of the multiplier, summation and non-linearity function in Figure 2 is equivalent to a node in Figure 1.</p>

<p><img src="/assets/img/nn/nn-detail.png" alt="raw" class="center-image" /></p>
<center><h4>Figure 2: Equivalent detailed view of the toy neural network</h4></center>

<p><script type="math/tex">z_1^{(2)}</script> is z of  node-1 in layer-2 (<em>Note: subscript =&gt; item number and superscript =&gt; layer-number</em>) and <script type="math/tex">a_1^{(2)}</script> is the activation score of node-1 in layer-2. Following equation is used to compute <script type="math/tex">z_i^{(j)}</script> and <script type="math/tex">a_i^{(j)}</script> in the figure 2 <br />
<script type="math/tex">z_1^{(2)} = x_1 * w_1 + x_2 * w_3</script>            <script type="math/tex">z_2^{(2)} = x_1 * w_2 + x_2 * w_4</script> <br />
<script type="math/tex">a_1^{(2)} = f(z_1^{(2)})</script>                                   <script type="math/tex">a_2^{(2)} = f(z_2^{(2)})</script> <br />
<script type="math/tex">z_1^{(3)} = a_1^{(2)} * w_5 + a_1^{(2)} * w_6</script>           <script type="math/tex">a_1^{(3)} = f(z_1^{(3)}) = \hat{y}</script> <br /></p>

<p>where, f() is a non-linearity function. Usually, non-linear function f() are <strong>sigmoid, tanh or Relu</strong>.</p>

<p>Backpropagation is all about computing partital derivative efficiently. Using backpropagation,  we can calculate <script type="math/tex">\frac{\partial E}{\partial w_i}</script> effectively where i = {1, 2, 3, 4, 5, 6}.</p>

<p>The partial derivative of error with respect to weight gives <strong>amount by which error will change if there is a small change in the weight</strong>. To calcualate the partial derivative of error w.r.t. weight  <script type="math/tex">\frac{\partial E}{\partial w_1}</script> <strong>chain rule</strong> is used.</p>

<p><script type="math/tex">\frac{\partial E}{\partial w_1} = \frac{\partial E}{\partial a_1^{(3)}} \frac{\partial a_1^{(3)}}{\partial w_1}</script> <br /></p>

<p><script type="math/tex">= \frac{\partial E}{\partial a_1^{(3)}} \frac{\partial a_1^{(3)}}{\partial z_1^{(3)}}   \frac{\partial z_1^{(3)}}{\partial w_1}</script>
<br />
<script type="math/tex">= \frac{\partial E}{\partial a_1^{(3)}} \frac{\partial a_1^{(3)}}{\partial z_1^{(3)}}   \frac{\partial z_1^{(3)}}{\partial a_1^{(2)}} \frac{\partial a_1^{(2)}}{\partial w_1}</script>  <br />
<script type="math/tex">= \frac{\partial E}{\partial a_1^{(3)}} \frac{\partial a_1^{(3)}}{\partial z_1^{(3)}}   \frac{\partial z_1^{(3)}}{\partial a_1^{(2)}} \frac{\partial a_1^{(2)}}{\partial z_1^{(2)}} \frac{\partial z_1^{(2)}}{\partial w_1}</script></p>

<p>To come up with the expression from Figure 1 can be confusing. However, if we construct a detailed view of the neural network as in Figure 2 then getting the expression is simply following chain rule from <strong>output to input</strong> (note it is backward). For <script type="math/tex">w_2</script> to <script type="math/tex">w_6</script>, the partial derivatives of error with respect to the weights can be calculated in the same way.</p>

<p>OMG!! expression of such a simple neural network is so complex. There will be total six partial derivatives, so how can we compute them effectively? <strong>The answer is dynamic programming.</strong>  Dynamic programming is the fancy name of ** memorization and reutilization of computation.** We use the <strong>chain rule</strong> to get the expression of the partial derivative of error w.r.t. weights. To compute such complex expressions efficiently  <strong>dynamic programming</strong> is used.</p>

<hr />
<h4 id="layer-3">Layer-3</h4>
<p><script type="math/tex">E =\frac{(y-\hat{y})^2}{2} =\frac{(y-a_1^{(3)})^2}{2}</script></p>

<script type="math/tex; mode=display">\frac{\partial E}{\partial z_1^{(3)}}  = \frac{\partial E}{\partial a_1^{(3)}} \frac{\partial a_1^{(3)}}{\partial z_1^{(3)}} = \frac{\partial \frac{(y-\hat{y})^2}{2}}{\partial a_1^{(3)}} \frac{\partial a_1^{(3)}}{\partial z_1^{(3)}} = \frac{\partial \frac{(y-a_1^{(3)})^2}{2}}{\partial a_1^{(3)}} \frac{\partial f(z_1^{(3)})}{\partial z_1^{(3)}} = -(y-a_1^{(3)}) \frac{\partial a_1^{(3)}}{\partial z_1^{(3)}}</script>

<script type="math/tex; mode=display">= (a_1^{(3)} -y) f'(z_1^{(3)}) = \delta_1^{(3)}</script>

<p>For ease of representation we represent <script type="math/tex">\frac{\partial E}{\partial z_1^{(3)}}</script> as <script type="math/tex">\delta_1^{(3)}</script>. <strong>Note that <script type="math/tex">z</script> and <script type="math/tex">\delta</script> have same sub-script (item-number) and superscript (layer-number).</strong></p>

<p>To evaluate  <script type="math/tex">\frac{\partial E}{\partial w_5}</script>, the computation of <script type="math/tex">\delta_1^{(3)}</script> can be utilized (memorized and reutilized) hence the repeated computation is avoided.</p>

<p><script type="math/tex">\frac{\partial E}{\partial w_5}=  \frac{\partial E}{\partial z_1^{(3)}} \frac{\partial z_1^{(3)}}{\partial w_5} = \delta_1^{(3)} \frac{\partial (a_1^{(2)} * w_5 + a_2^{(2)} * w_6)}{\partial w_5} = \delta_1^{(3)} a_1^{(2)}</script>
First chain rule is used to obtain the expression and then computaiton of <script type="math/tex">\delta_1^{(3)}</script> is utilzed to compute  <script type="math/tex">\frac{\partial E}{\partial w_5}</script>. Following the similar steps, <script type="math/tex">\frac{\partial E}{\partial w_6}</script> can be computed.
  <script type="math/tex">\frac{\partial E}{\partial w_6}= \delta_1^{(3)} a_2^{(2)}</script></p>

<p>### Layer-2
The computations of layer-3 is reused for the computation of layer 2. Note that, computation starts from output toward input using the chain rule. The reutilization of the computation can be seen as message passing in the backward direction.</p>

<script type="math/tex; mode=display">\frac{\partial E}{\partial z_1^{(2)}}  = \frac{\partial E}{\partial z_1^{(3)}} \frac{\partial z_1^{(3)}}{\partial a_1^{(3)}} \frac{\partial a_1^{(2)}}{\partial z_1^{(2)}} = \delta_1^{(3)}  \frac{\partial (a_1^{(2)} * w_5 + a_1^{(2)} * w_6)}{\partial a_1^{(2)}}  \frac{\partial f(z_1^{(2)})}{\partial z_1^{(2)}} = \delta_1^{(3)} * w_5 *f'(z_1^{(2)}) = \delta_1^{(2)}</script>

<p>Similarly,
<script type="math/tex">\frac{\partial E}{\partial z_2^{(2)}} = \delta_1^{(3)} * w_6 * f'(z_2^{(2)}) = \delta_2^{(2)}</script></p>

<p>It is observed that term which appears in the computation of <script type="math/tex">\frac{\partial E}{\partial w_5}</script> will be memorized and reutilized in the computation of  <script type="math/tex">\frac{\partial E}{\partial w_1}</script>.</p>

<p>Let’s calculate the partial derivative of error with respect to weight.</p>

<script type="math/tex; mode=display">\frac{\partial E}{\partial w_1} = \frac{\partial E}{\partial z_1^{(2)}} \frac{\partial  (x_1 * w_1 + x_2 * w_3)}{\partial w_1} =  \delta_1^{(2)} * x_1</script>

<p>let’s say <script type="math/tex">x_1 = a_1^{(1)}</script> and  <script type="math/tex">x_2 = a_2^{(1)}</script>
Then,  <script type="math/tex">\frac{\partial E}{\partial w_1}  =  \delta_1^{(2)} * a_1^{(1)}</script></p>

<p>We start with  following expression,
<script type="math/tex">\frac{\partial E}{\partial w_1}  = \frac{\partial E}{\partial a_1^{(3)}} \frac{\partial a_1^{(3)}}{\partial z_1^{(3)}}   \frac{\partial z_1^{(3)}}{\partial a_1^{(2)}} \frac{\partial a_1^{(2)}}{\partial z_1^{(2)}} \frac{\partial z_1^{(2)}}{\partial w_1}</script> which looks like there is a lot of computation. But, utilizing the dynamic programming in each layer computation is minimized. Now,  <script type="math/tex">\frac{\partial E}{\partial w_1}  =  \delta_1^{(2)} * a_1^{(1)}</script> which looks much simpler and also computationally very efficient.</p>

<p>Similarly, the expression for the partial derivative of error w.r.t. remaining weights can be computed. We observed that if we break down a neural network into a detailed diagram as in Figure 2 then it would be easier to come up with <strong>chain rule</strong>. Once we have the expression, the long and complex expression can be computed efficiently using <strong>dynamic programming</strong>.</p>


  </article>

  

</div>

      </div>
    </div>

    <footer>

  <div class="wrapper">
    &copy; Copyright 2018 Surendra Sedhai.
    Powered by <a href="http://jekyllrb.com/" target="_blank">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio">al-folio</a> theme.

    
  </div>

</footer>


    <!-- Load jQuery -->
<script src="//code.jquery.com/jquery-1.12.4.min.js"></script>

<!-- Load Common JS -->
<script src="http://localhost:4000/assets/js/common.js"></script>


<!-- Load KaTeX -->
<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/katex.min.css">
<script src="//cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/katex.min.js"></script>
<script src="http://localhost:4000/assets/js/katex.js"></script>




<!-- Include custom icon fonts -->
<link rel="stylesheet" href="http://localhost:4000/assets/css/font-awesome.min.css">
<link rel="stylesheet" href="http://localhost:4000/assets/css/academicons.min.css">

<!-- Google Analytics -->
<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-XXXXXXXX-X', 'auto');
ga('send', 'pageview');
</script>


  </body>

</html>
