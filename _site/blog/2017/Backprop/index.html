<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge">

  <title>Surendra Sedhai | Backpropagation Demystified</title>
  <meta name="description" content="Lets explore :) .
">

  <link rel="shortcut icon" href="http://localhost:4000/assets/img/favicon.ico">

  <link rel="stylesheet" href="http://localhost:4000/assets/css/main.css">
  <link rel="canonical" href="http://localhost:4000/blog/2017/Backprop/">
</head>


  <body>

    <header class="site-header">

  <div class="wrapper">

    
    <span class="site-title">
        
        <a href="/" style="color:white"><strong>Surendra</strong>  Sedhai</a>
    </span>
    

    <nav class="site-nav" >

      <div class="trigger" style="color:white">
        <!-- About -->
        <a class="page-link"  style="color:white" href="http://localhost:4000/">About</a>

        <!-- Blog -->
        <a class="page-link" style="color:white" href="http://localhost:4000/blog/">Blog</a>

        <!-- Pages -->
        
          
        
          
        
          
        
          
            <a class="page-link"  style="color:white"  href="http://localhost:4000/publications/">Publications</a>
          
        
          
        

        <!-- CV link -->
        <!-- <a class="page-link" href="http://localhost:4000/assets/pdf/CV.pdf">vitae</a> -->

      </div>
    </nav>

  </div>

</header>



    <div class="page-content">
      <div class="wrapper">
        <div class="post">

  <header class="post-header">
    
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>
<script
  type="text/javascript"
  charset="utf-8"
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"
>
</script>
<script
  type="text/javascript"
  charset="utf-8"
  src="https://vincenttam.github.io/javascripts/MathJaxLocal.js"
>
</script>


    <h1 class="post-title">Backpropagation Demystified</h1>
    <p class="post-meta">September 9, 2017</p>
  </header>

  <article class="post-content">
    <p>The main goal of training a neural network is to determine the value of each connection weight which minimize the specified cost function. Backpropagation is the standard way of training neural network where weights of the neural network connections are updated as follows</p>

<script type="math/tex; mode=display">w_i := w_i - \alpha *  \frac{\partial E(w_i)}{\partial w_i}</script>

<p>Learning rate controls how much we want to learn in each step of training process. Learning rate is determined empirically.</p>

<p>Partial derivative of cost function w.r.t. weight gives the direction in which weight should be changed to improve the performance of the system. Above expression shows that to update weight of the connection, the partial derivative of cost function $E(w_i)$ with respect to the weight <script type="math/tex">w_i</script> need to be computed. <strong>Backpropagation is an efficient way to compute partial derivative of error w.r.t. each connection weight of the neural network</strong>.</p>

<h3 id="refresher-intuitive-meaning-of-derivative-and-partial-derivative">Refresher: Intuitive meaning of derivative and partial derivative</h3>
<p>Let’s assume <script type="math/tex">x_1</script> is time allocated by a man for exercise and <script type="math/tex">y</script> is the weight of the man. The derivative of <script type="math/tex">y</script>  w.r.t.  <script type="math/tex">x_1</script> means if the man makes a small change in the time allocated for the exercise how much his weight will change. Mathematically, the derivative of <script type="math/tex">y</script> w.r.t.  <script type="math/tex">x_1</script> is represented as <script type="math/tex">\frac{dy}{dx_1}</script></p>

<p>We know that diet (<script type="math/tex">x_2</script>) also has an effect on the weight of the person.  Analysis of change in the weight of the person w.r.t. a  small change in the exercise keeping diet same is given by the partial derivative of <script type="math/tex">y</script>   w.r.t.   <script type="math/tex">x_1</script>. Mathematically, the partial derivative of <script type="math/tex">y</script> w.r.t. <script type="math/tex">\large x_1</script> is represented as <script type="math/tex">\frac{\partial y}{\partial x_1}</script>. Similarly, if the person keeps the time of exercise <script type="math/tex">x_1</script> same and makes a small change in the diet <script type="math/tex">x_2</script> it is represented as the partial derivative of <script type="math/tex">y</script> w.r.t. <script type="math/tex">x_2</script>. Partial derivative of <script type="math/tex">y</script>   w.r.t.   <script type="math/tex">x_2</script> is represented as <script type="math/tex">\frac{\partial y}{\partial x_2}</script></p>

<p><img src="/assets/img/nn/raw-nn.png" alt="raw" class="center-image" /></p>
<center><h4>Figure 1: Toy neural network</h4></center>

<p>Figure 1 shows one of the simplest possible neural network which is good enough to understand details of the backpropagation algorithm. We consider input-layer as layer-1 (<script type="math/tex">L_1</script>), hidden-layer as (<script type="math/tex">L_2</script>) and output-layer as (<script type="math/tex">L_3</script>). <script type="math/tex">x_1</script>  and <script type="math/tex">x_2</script> are input, blue nodes are nodes of hidden-layer and the red node is output-node. There are 6 connection weights <script type="math/tex">w_1</script> to <script type="math/tex">w_6</script>.</p>

<h4 id="we-want-to-minimize-the-square-of-error">We want to minimize the square of error</h4>

<p>Square Error  (E) = <script type="math/tex">\frac{(y-\hat{y})^2}{2}</script></p>

<p><em>Square of error is divided by 2 just to make computation clean and we know that constant does not affect optimization problem.</em></p>

<p>A node (blue/red) in Figure 1 represents three computations namely,  multiplication of an input and a weight, sum of all the results of multiplications and finally activation score which is non-linearity of the summation. To have a better insight about the neural network, we redraw a detailed diagram in Figure 2. Multiplier, summation and non-linearity function in Figure 2 is equivalent to a node in Figure 1.</p>

<p><img src="/assets/img/nn/nn-detail.png" alt="raw" class="center-image" /></p>
<center><h4>Figure 2: Equivalent detailed view of the toy neural network</h4></center>

<p>The way to read <script type="math/tex">z_1^{(2)}</script> is z of  node-1 in layer-2 [<strong>Note: subscript =&gt; item number and superscript =&gt; layer-number</strong>] . <script type="math/tex">a_1^{(2)}</script> is the activation score of node-1 in layer-2. Following expressions can be obtained from figure 2 <br />
<script type="math/tex">z_1^{(2)} = x_1 * w_1 + x_2 * w_3</script>            <script type="math/tex">z_2^{(2)} = x_1 * w_2 + x_2 * w_4</script> <br />
<script type="math/tex">a_1^{(2)} = f(z_1^{(2)})</script>                                   <script type="math/tex">a_2^{(2)} = f(z_2^{(2)})</script> <br />
<script type="math/tex">z_1^{(3)} = a_1^{(2)} * w_5 + a_1^{(2)} * w_6</script>           <script type="math/tex">a_1^{(3)} = f(z_1^{(3)}) = \hat{y}</script> <br /></p>

<p>where, f() is a non-linearity function. Usually, non-linear function f() are <strong>sigmoid, tanh or Relu</strong>.</p>

<p>Backpropagation is all about computing partital derivative efficiently. Using backpropagation,  we can calculate <script type="math/tex">\frac{\partial E}{\partial w_i}</script> effectively where i = {1, 2, 3, 4, 5, 6}.</p>

<p>The partial derivative of error with respect to weight gives <strong>amount by which error will change if there is a small change in the weight</strong>. To calcualate the partial derivative of error w.r.t. weight  <script type="math/tex">\frac{\partial E}{\partial w_1}</script> <strong>chain rule</strong> is used.</p>

<script type="math/tex; mode=display">\frac{\partial E}{\partial w_1} = \frac{\partial E}{\partial a_1^{(3)}} \frac{\partial a_1^{(3)}}{\partial w_1} = \frac{\partial E}{\partial a_1^{(3)}} \frac{\partial a_1^{(3)}}{\partial z_1^{(3)}}   \frac{\partial z_1^{(3)}}{\partial w_1} = \frac{\partial E}{\partial a_1^{(3)}} \frac{\partial a_1^{(3)}}{\partial z_1^{(3)}}   \frac{\partial z_1^{(3)}}{\partial a_1^{(2)}} \frac{\partial a_1^{(2)}}{\partial w_1}  = \frac{\partial E}{\partial a_1^{(3)}} \frac{\partial a_1^{(3)}}{\partial z_1^{(3)}}   \frac{\partial z_1^{(3)}}{\partial a_1^{(2)}} \frac{\partial a_1^{(2)}}{\partial z_1^{(2)}} \frac{\partial z_1^{(2)}}{\partial w_1}</script>

<p>To come up with the expression from Figure 1 can be confusing. However, if we construct a detailed view of the neural network as in Figure 2 then getting the expression is simply following chain rule from <strong>output to input</strong> (note it is backward). For <script type="math/tex">w_2</script> to <script type="math/tex">w_6</script>, the partial derivatives of error with respect to the weights can be calculated in the same way.</p>

<p>OMG!! expression of such a simple neural network is so complex. There will be total six partial derivatives, so how can we compute them effectively? <strong>The answer is dynamic programming.</strong>  Dynamic programming is the fancy name of ** memorization and reutilization of computation.** We use the <strong>chain rule</strong> to get the expression of the partial derivative of error w.r.t. weights. To compute such complex expressions efficiently  <strong>dynamic programming</strong> is used.</p>

<hr />
<h4 id="layer-3">Layer-3</h4>
<p><script type="math/tex">E =\frac{(y-\hat{y})^2}{2} =\frac{(y-a_1^{(3)})^2}{2}</script></p>

<script type="math/tex; mode=display">\frac{\partial E}{\partial z_1^{(3)}}  = \frac{\partial E}{\partial a_1^{(3)}} \frac{\partial a_1^{(3)}}{\partial z_1^{(3)}} = \frac{\partial \frac{(y-\hat{y})^2}{2}}{\partial a_1^{(3)}} \frac{\partial a_1^{(3)}}{\partial z_1^{(3)}} = \frac{\partial \frac{(y-a_1^{(3)})^2}{2}}{\partial a_1^{(3)}} \frac{\partial f(z_1^{(3)})}{\partial z_1^{(3)}} = -(y-a_1^{(3)}) \frac{\partial a_1^{(3)}}{\partial z_1^{(3)}}</script>

<script type="math/tex; mode=display">= (a_1^{(3)} -y) f'(z_1^{(3)}) = \delta_1^{(3)}</script>

<p>For ease of representation we represent <script type="math/tex">\frac{\partial E}{\partial z_1^{(3)}}</script> as <script type="math/tex">\delta_1^{(3)}</script>. <strong>Note that <script type="math/tex">z</script> and <script type="math/tex">\delta</script> have same sub-script (item-number) and superscript (layer-number).</strong></p>

<p>To evaluate  <script type="math/tex">\frac{\partial E}{\partial w_5}</script>, the computation of <script type="math/tex">\delta_1^{(3)}</script> can be utilized (memorized and reutilized) hence the repeated computation is avoided.</p>

<p><script type="math/tex">\frac{\partial E}{\partial w_5}=  \frac{\partial E}{\partial z_1^{(3)}} \frac{\partial z_1^{(3)}}{\partial w_5} = \delta_1^{(3)} \frac{\partial (a_1^{(2)} * w_5 + a_2^{(2)} * w_6)}{\partial w_5} = \delta_1^{(3)} a_1^{(2)}</script>
First chain rule is used to obtain the expression and then computaiton of <script type="math/tex">\delta_1^{(3)}</script> is utilzed to compute  <script type="math/tex">\frac{\partial E}{\partial w_5}</script>. Following the similar steps, <script type="math/tex">\frac{\partial E}{\partial w_6}</script> can be computed.
  <script type="math/tex">\frac{\partial E}{\partial w_6}= \delta_1^{(3)} a_2^{(2)}</script></p>

<p>### Layer-2
The computations of layer-3 is reused for the computation of layer 2. Note that, computation starts from output toward input using the chain rule. The reutilization of the computation can be seen as message passing in the backward direction.</p>

<script type="math/tex; mode=display">\frac{\partial E}{\partial z_1^{(2)}}  = \frac{\partial E}{\partial z_1^{(3)}} \frac{\partial z_1^{(3)}}{\partial a_1^{(3)}} \frac{\partial a_1^{(2)}}{\partial z_1^{(2)}} = \delta_1^{(3)}  \frac{\partial (a_1^{(2)} * w_5 + a_1^{(2)} * w_6)}{\partial a_1^{(2)}}  \frac{\partial f(z_1^{(2)})}{\partial z_1^{(2)}} = \delta_1^{(3)} * w_5 *f'(z_1^{(2)}) = \delta_1^{(2)}</script>

<p>Similarly,
<script type="math/tex">\frac{\partial E}{\partial z_2^{(2)}} = \delta_1^{(3)} * w_6 * f'(z_2^{(2)}) = \delta_2^{(2)}</script></p>

<p>It is observed that term which appears in the computation of <script type="math/tex">\frac{\partial E}{\partial w_5}</script> will be memorized and reutilized in the computation of  <script type="math/tex">\frac{\partial E}{\partial w_1}</script>.</p>

<p>Let’s calculate the partial derivative of error with respect to weight.</p>

<script type="math/tex; mode=display">\frac{\partial E}{\partial w_1} = \frac{\partial E}{\partial z_1^{(2)}} \frac{\partial  (x_1 * w_1 + x_2 * w_3)}{\partial w_1} =  \delta_1^{(2)} * x_1</script>

<p>let’s say <script type="math/tex">x_1 = a_1^{(1)}</script> and  <script type="math/tex">x_2 = a_2^{(1)}</script>
Then,  <script type="math/tex">\frac{\partial E}{\partial w_1}  =  \delta_1^{(2)} * a_1^{(1)}</script></p>

<p>We start with  following expression,
<script type="math/tex">\frac{\partial E}{\partial w_1}  = \frac{\partial E}{\partial a_1^{(3)}} \frac{\partial a_1^{(3)}}{\partial z_1^{(3)}}   \frac{\partial z_1^{(3)}}{\partial a_1^{(2)}} \frac{\partial a_1^{(2)}}{\partial z_1^{(2)}} \frac{\partial z_1^{(2)}}{\partial w_1}</script> which looks like there is a lot of computation. But, utilizing the dynamic programming in each layer computation is minimized. Now,  <script type="math/tex">\frac{\partial E}{\partial w_1}  =  \delta_1^{(2)} * a_1^{(1)}</script> which looks much simpler and also computationally very efficient.</p>

<p>Similarly, the expression for the partial derivative of error w.r.t. remaining weights can be computed. We observed that if we break down a neural network into a detailed diagram as in Figure 2 then it would be easier to come up with <strong>chain rule</strong>. Once we have the expression, the long and complex expression can be computed efficiently using <strong>dynamic programming</strong>.</p>


  </article>

  

</div>

      </div>
    </div>

    <footer>

  <div class="wrapper">
    &copy; Copyright 2018 Surendra Sedhai.
    Powered by <a href="http://jekyllrb.com/" target="_blank">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio">al-folio</a> theme.

    
  </div>

</footer>


    <!-- Load jQuery -->
<script src="//code.jquery.com/jquery-1.12.4.min.js"></script>

<!-- Load Common JS -->
<script src="http://localhost:4000/assets/js/common.js"></script>


<!-- Load KaTeX -->
<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/katex.min.css">
<script src="//cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/katex.min.js"></script>
<script src="http://localhost:4000/assets/js/katex.js"></script>




<!-- Include custom icon fonts -->
<link rel="stylesheet" href="http://localhost:4000/assets/css/font-awesome.min.css">
<link rel="stylesheet" href="http://localhost:4000/assets/css/academicons.min.css">

<!-- Google Analytics -->
<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-XXXXXXXX-X', 'auto');
ga('send', 'pageview');
</script>


  </body>

</html>
